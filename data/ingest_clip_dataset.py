"""
Ingest the Clip Images Data (or a similar folder of images+captions) into the vector store.

Expected structure options:
- images/ and captions.txt where each line is: <filename>\t<caption>
- or a folder with images only; captions will be generated by the model (requires GEMINI_API_KEY)

Usage (example):
  python -m data.ingest_clip_dataset --root "C:\\path\\to\\dataset" --user demo_user --lang en

Notes: Uses the same embedding and DB utilities as the API, running on CPU.
"""
import argparse
import os
from pathlib import Path
import base64
from datetime import datetime

from backend.db.vector_store import ensure_collection, upsert_point
from backend.services.embeddings import clip_image_embedding, multilingual_text_embedding
from backend.services.generation import configure_gemini, generate_description


def load_captions(captions_file: Path) -> dict:
    caps = {}
    if not captions_file.exists():
        return caps
    for line in captions_file.read_text(encoding="utf-8").splitlines():
        parts = line.split("\t", 1)
        if len(parts) == 2:
            caps[parts[0].strip()] = parts[1].strip()
    return caps


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", required=True, help="Dataset root folder")
    ap.add_argument("--user", default="demo_user", help="User ID to assign")
    ap.add_argument("--lang", default="en", help="Base language for captions")
    ap.add_argument("--enable_clip", action="store_true", help="Compute CLIP vectors (default true if ENV ENABLE_CLIP=1)")
    args = ap.parse_args()

    root = Path(args.root)
    images_dir = root / "images"
    captions_file = root / "captions.txt"
    if not images_dir.exists():
        images_dir = root

    ENABLE_CLIP = os.getenv("ENABLE_CLIP", "0") == "1" or args.enable_clip
    ensure_collection()

    # Optional Gemini captioning
    gem_key = os.getenv("GEMINI_API_KEY")
    if gem_key:
        configure_gemini(gem_key)

    captions = load_captions(captions_file)

    count = 0
    for p in images_dir.iterdir():
        if p.suffix.lower() not in {".jpg", ".jpeg", ".png"}:
            continue
        try:
            content_bytes = p.read_bytes()
            clip_vec = clip_image_embedding(content_bytes) if ENABLE_CLIP else [0.0] * 512
            caption = captions.get(p.name)
            if not caption:
                caption = generate_description(content_bytes, args.lang)
            text_vec = multilingual_text_embedding(caption)
            payload = {
                "user_id": args.user,
                "lang": args.lang,
                "timestamp": datetime.now().isoformat(),
                "type": "image",
                "original_name": p.name,
                "content": caption,
                "image_b64": base64.b64encode(content_bytes).decode("utf-8"),
            }
            vectors = {"clip": clip_vec, "text": text_vec}
            upsert_point(f"{args.user}:{p.name}", vectors, payload)
            count += 1
        except Exception as e:
            print(f"Skip {p.name}: {e}")

    print(f"Ingested {count} items from {images_dir}")


if __name__ == "__main__":
    main()
